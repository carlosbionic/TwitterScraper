{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises from\n",
    "# https://www.w3resource.com/python-exercises/web-scraping/index.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a Python program to test if a given page is found or not on the server.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from urllib.error import URLError\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "web_page1 = 'https://www.w3resource.com/python-exercises/web-scraping/index.php'\n",
    "web_page2 = 'http://www.example.com/'\n",
    "web_page3 = 'http://aksldfakdf.com'\n",
    "\n",
    "try:\n",
    "    html = urlopen(web_page2)\n",
    "except HTTPError as e:\n",
    "    print('HTTP error\\n')\n",
    "except URLError as e:\n",
    "    print('URL error; server not found\\n')\n",
    "else:\n",
    "    print('Page loaded succesfully\\n')\n",
    "    soup = bs(html)              # Make beautifulsoup to the data already read\n",
    "    prettyHTML = soup.prettify() # prettify the html\n",
    "    print(prettyHTML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Write a Python program to download and display the content of robot.txt for en.wikipedia.org.\n",
    "\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://en.wikipedia.org/robots.txt')\n",
    "test = response.text\n",
    "print('robots.txt from Wikipedia.org')\n",
    "print('==============================\\n')\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a Python program to get the number of datasets currently listed on data.gov.\n",
    "\n",
    "from lxml import html\n",
    "import requests\n",
    "\n",
    "response = requests.get('https://www.usa.gov/')\n",
    "#print(response.text)\n",
    "doc_gov = html.fromstring(response.text)\n",
    "#print(doc_gov)\n",
    "link_gov = doc_gov.cssselect('small a')[0]\n",
    "print(\"Number of datasets currently listed on data.gov:\")\n",
    "print(link_gov.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "4. Write a Python program to convert an address (like \"1600 Amphitheatre Parkway, Mountain View, CA\") \n",
    "into geographic coordinates (like latitude 37.423021 and longitude -122.083739). \n",
    "Geocodin: Geocoding is the process of converting addresses (like \"1600 Amphitheatre Parkway, Mountain View, CA\") \n",
    "into geographic coordinates (like latitude 37.423021 and longitude -122.083739), \n",
    "which you can use to place markers on a map, or position the map.\n",
    "'''\n",
    "\n",
    "import requests\n",
    "\n",
    "geo_url = 'http://maps.googleapis.com/maps/api/geocode/json'\n",
    "my_address = {'address': '21 Ramkrishana Road, Burdwan, East Burdwan, West Bengal, India', \n",
    "             'language': 'en'}\n",
    "response = requests.get(geo_url, params = my_address)\n",
    "print(response.json())\n",
    "results = response.json()['results']\n",
    "print(results)\n",
    "my_geo = results[0]['geometry']['location']\n",
    "print(\"Longitude:\",my_geo['lng'],\"\\n\",\"Latitude:\",my_geo['lat'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write a Python program to extract h1 tag from example.com.\n",
    "\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "my_url = 'http://www.example.com/'\n",
    "uClient = uReq(my_url)\n",
    "html = uClient.read()\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "print('Complete html page:\\n')\n",
    "print(soup)\n",
    "\n",
    "print('h1 tag:\\n')\n",
    "print(soup.h1.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Write a Python program to extract and display all the header tags from en.wikipedia.org/wiki/Main_Page.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "wiki_page = urlopen('https://en.wikipedia.org/wiki/Main_Page')\n",
    "html = wiki_page.read()\n",
    "soup = bs(html, 'html.parser')\n",
    "#print(soup.prettify)\n",
    "\n",
    "titles = soup.find_all(['h1','h2','h3','h4','h5','h6'])\n",
    "print(*titles, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write a Python program to extract and display \n",
    "# all the image links from en.wikipedia.org/wiki/Peter_Jeffrey_(RAAF_officer).\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "string_page = 'https://en.wikipedia.org/wiki/Peter_Jeffrey_(RAAF_officer)'\n",
    "wiki_page = urlopen(string_page)\n",
    "html = wiki_page.read()\n",
    "soup = bs(html, 'html.parser')\n",
    "#soup.prettify\n",
    "\n",
    "images = soup.findAll('img',{'src':re.compile('.png|.jpg')})\n",
    "\n",
    "#print(*images, sep='\\n\\n')\n",
    "\n",
    "for image in images:\n",
    "    print(image['src'] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Understanding regular expressions (regex)\n",
    "\n",
    "import re\n",
    "\n",
    "pattern = re.compile(r\"\\w\") # compile the regex pattern which will be used for matching later\n",
    "                            # \\w matches alphanumeric character [a-zA-Z0-9_]\n",
    "                            # \\w sequence matches only the first letter or digit at the start of the string\n",
    "\n",
    "string = 'regex is awesome!'\n",
    "result = pattern.match(string) # calling a matching method to match our pattern\n",
    "print(result.group())          # group() is a match object method that returns an entire match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Write a Python program to that retrieves an arbitary Wikipedia page of \"Python\" \n",
    "# and creates a list of links on that page.\n",
    "\n",
    "from urllib.request import urlopen as uReq\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "wikiStr = 'https://en.wikipedia.org/wiki/Python'\n",
    "page = uReq(wikiStr)\n",
    "html = page.read()\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "links = soup.findAll('a')\n",
    "\n",
    "for link in links:\n",
    "    if 'href' in link.attrs:\n",
    "        print(link.attrs['href'])\n",
    "        \n",
    "#print(*links, sep='\\n\\n')\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Write a Python program to check whether a page contains a title or not.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from urllib.error import HTTPError\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def getTitle(url):\n",
    "\n",
    "    try:\n",
    "        html = urlopen(url)\n",
    "    except HTTPError as e:\n",
    "        print('The page could not be opened')\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    try:\n",
    "        soup = BeautifulSoup(html.read(), 'html.parser')\n",
    "        title = soup.body.h1\n",
    "    except AttributeError as e:\n",
    "        print('Title not found')\n",
    "        print(e)\n",
    "        return None\n",
    "    \n",
    "    return title\n",
    "\n",
    "print(getTitle('http://www.example.com/'))\n",
    "print()\n",
    "print(getTitle('https://www.w3resource.com/'))\n",
    "print()\n",
    "print(getTitle('https://en.wikipedia.org/wiki/Main_Page'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Write a Python program to list all language names and number \n",
    "# of related articles in the order they appear in wikipedia.org.\n",
    "\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "html = urlopen('https://www.wikipedia.org/')\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "nameList = soup.findAll('a',{'class':'link-box'})\n",
    "for name in nameList:\n",
    "    print(name.get_text())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13. Write a Python program to get the number of people visiting a U.S. government website right now.\n",
    "# Source: https://analytics.usa.gov/data/live/realtime.json\n",
    "\n",
    "import requests\n",
    "\n",
    "url = 'https://analytics.usa.gov/data/live/realtime.json'\n",
    "j = requests.get(url).json()\n",
    "\n",
    "print(\"Number of people visiting a U.S. government website-\")\n",
    "print(\"Active Users Right Now:\")\n",
    "print(j['data'][0]['active_visitors'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Write a Python program get the number of security alerts issued by US-CERT in the current year.\n",
    "# Source: https://www.us-cert.gov/ncas/alerts\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://www.us-cert.gov/ncas/alerts/2018'\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "items = soup.find('div',{'class':'item-list'}).findAll('li')\n",
    "print(len(items))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Write a Python program to get the number of Pinterest accounts \n",
    "# maintained by U.S. State Department embassies and missions.\n",
    "# Source: https://www.state.gov/r/pa/ode/socialmedia/\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "url = 'https://www.state.gov/r/pa/ode/socialmedia/'\n",
    "html = requests.get(url).text\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "# http://pinterest.com/usembassynz/\n",
    "pinterestAccs = soup.findAll('a',{'href':re.compile('http://pinterest.com/')})\n",
    "print('Number of Pinterest accounts:')\n",
    "print(len(pinterestAccs))\n",
    "\n",
    "faceAccs = soup.findAll('a',{'href':re.compile('http://www.facebook.com/')})\n",
    "print('Number of Facebook accounts:')\n",
    "print(len(faceAccs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets: 258 \n",
      "Number of following: 24 \n",
      "Number of followers: 18 \n"
     ]
    }
   ],
   "source": [
    "# 16. Write a Python program to get the number of followers of a given twitter account.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#handle = input('Enter your Twitter account name: ')\n",
    "handle = 'ProfeCarlos1'\n",
    "html = requests.get('https://twitter.com/'+handle).text\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "try:\n",
    "    tweets = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "    numTweets = tweets.find('span',{'class':'ProfileNav-value'}).get('data-count')\n",
    "    \n",
    "    following = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--following'})\n",
    "    numFollowing = following.find('span',{'class':'ProfileNav-value'}).get('data-count')\n",
    "    \n",
    "    followers = soup.find('li',{'class':'ProfileNav-item ProfileNav-item--followers'})\n",
    "    numFollowers = followers.find('span',{'class':'ProfileNav-value'}).get('data-count')\n",
    "    \n",
    "    print('Number of tweets: {} '.format(numTweets))\n",
    "    print('Number of following: {} '.format(numFollowing))\n",
    "    print('Number of followers: {} '.format(numFollowers))\n",
    "except:\n",
    "    print('Account name not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "*****************************************************\n",
      "Clases Particulares Matemática, Física Y Química A Domicilio - $ 300,00https://servicio.mercadolibre.com.ar/MLA-769577340-clases-particulares-matematica-fisica-y-quimica-a-domicilio-_JM …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Hola a todos. Espero que hayan empezado el año muy bien!\n",
      "\n",
      "Les comparto un primer video que hice explicando un ejercicio de integrales. Es un video de prueba, así que con gusto acepto todo tipo de comentarios, sugerencias, críticas, opiniones y lo que...https://www.youtube.com/watch?v=q__DsOEIZ_g …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/2180584021993276 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "AUTOMATICA - Robots Vs. Music - Nigel Stanfordhttps://www.youtube.com/watch?v=bAdqazixuRY&fbclid=IwAR2ct8LlcwQWacEG2-ytqVAAS_2743ti0FrEq9X4zrL7FS_sV_70lqE5PvE …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "CYMATICS: Science Vs. Music - Nigel Stanfordhttps://www.youtube.com/watch?v=Q3oItpVa9fs&fbclid=IwAR0JjtSs79HOtH4YNR0RhO1UDQqTfJ4KcniMlcz6Nb7Q0W-nv6MVfSL-mJE …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/2057257464325933 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/2022862267765453 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/2002746973110316 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/elartedelaprogramacion/videos/789293887922580/?hc_ref=ARSFlbcJGcnpcT-xoxV6FxafRzgT7_GYyvw6s1cV3XfhLt_tERZTju68WYPgd3HW0XI …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Mucha suerte a todos en estos últimos exámenes de mitad de año. A ponerse las pilas. Un último esfuerzo !!!\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/1853968821321466 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/1853968181321530 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/1853966894654992 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "https://www.facebook.com/ProfeCarlosPrado/posts/1822379414480407 …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Un físico argentino obtuvo un prestigioso premio que lo convierte en candidato al Nobelhttps://www.infobae.com/salud/ciencia/2018/03/27/un-fisico-argentino-obtuvo-un-prestigioso-premio-que-lo-convierte-en-candidato-al-nobel/ …\n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Descubren el número primo más largo del mundohttps://fb.me/9aW5VHrkS \n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Steven Strogatz and Hilbert's Infinite Hotel http://fb.me/1hrYIvc9Y \n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Atractor para un mapa logístico.http://fb.me/86SAJaowg \n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "http://fb.me/15p7VHix8 \n",
      "*****************************************************\n",
      "\n",
      "*****************************************************\n",
      "Entre cometas, meteoros y planetas: los 10 increíbles eventos astronómicos de 2017http://fb.me/1HWZsjb7m \n",
      "*****************************************************\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 20. Write a Python program to scrap number of tweets of a given Twitter account.\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#handle = input('Enter your Twitter account name: ')\n",
    "handle = 'ProfeCarlos1'\n",
    "html = requests.get('https://twitter.com/'+handle).text\n",
    "\n",
    "soup = bs(html, 'html.parser')\n",
    "tweets = soup.findAll('div',{'class':'tweet'})\n",
    "print(len(tweets))\n",
    "\n",
    "for tweet in tweets:\n",
    "    print('*****************************************************')\n",
    "    print(tweet.find('p',{'class':'TweetTextSize TweetTextSize--normal js-tweet-text tweet-text'}).text)\n",
    "    print('*****************************************************\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
